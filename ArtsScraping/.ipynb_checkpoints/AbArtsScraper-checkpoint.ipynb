{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello friends, here is my artworks database for Abstract Value-based Decision Making project\n",
    "\n",
    "I'm going to be using Beautifulsoup to scrape the arts pictures.\n",
    "\n",
    "## First of all, import all required stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import json\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create directories for all of our artworks by style. Then download the artworks to the corresponding folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "# Create a list of artworks styles wanted\n",
    "styles = ['abstract-art', 'surrealism', 'abstract-expressionism', 'post-impressionism', 'pop-art']\n",
    "# Create a list container for search result urls\n",
    "searchresults = list()\n",
    "\n",
    "for style in styles:\n",
    "    path = cwd + '/' + style\n",
    "    # Create the sub-directory by style\n",
    "    os.makedirs(path)\n",
    "    \n",
    "    # Create the full url name with style-name appened\n",
    "    searchresult = \"https://www.wikiart.org/en/paintings-by-style/\" + style\n",
    "    searchresults.append(searchresult)\n",
    "    \n",
    "    # Change working directory to the style sub-directory created\n",
    "    os.chdir(path)\n",
    "    \n",
    "    # Let's get scrapin' baby!\n",
    "    for searchresult in searchresults:\n",
    "        # Request the content from the url\n",
    "        r = requests.get(searchresult)\n",
    "        # Get only the plain text component\n",
    "        data = r.text\n",
    "        # Parse it with through HTML parser\n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "    \n",
    "        # Upon speculations (using print(soup.prettify()), the links of pictures were stored in JSON format. \n",
    "        # The JSON of picture-urls is stored under <script> HTML tag. Let's grab it.\n",
    "        links = soup.find_all('script', type = \"text/plain\")\n",
    "        \n",
    "        # I'm not sure how to deal with JSON so I find another way around.\n",
    "        # Simply look for patterns and grap the urls of pictures using regular expression.\n",
    "        string = str(links)\n",
    "        pattern = 'https?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+.jpg'\n",
    "        pictures = re.findall(pattern, string)\n",
    "        \n",
    "        # Well now, I got the urls in a list format. Let's download them all!!\n",
    "        for picture in pictures:\n",
    "            # Get the time stamp of current time\n",
    "            timestamp = time.asctime() \n",
    "            # Create file names\n",
    "            txt = open('%s.jpg' % timestamp, \"wb\")\n",
    "            # Link the url\n",
    "            download_img = urllib.request.urlopen(picture)\n",
    "            # Download it!\n",
    "            txt.write(download_img.read())\n",
    "            txt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# VOIL√Å!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
